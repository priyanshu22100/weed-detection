{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training for ResNet50**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Libraries to import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preproccessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T07:33:29.798096Z",
     "iopub.status.busy": "2025-02-05T07:33:29.797895Z",
     "iopub.status.idle": "2025-02-05T07:33:44.027204Z",
     "shell.execute_reply": "2025-02-05T07:33:44.026328Z",
     "shell.execute_reply.started": "2025-02-05T07:33:29.798077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original dataset statistics:\n",
      "Total original images: 200\n",
      "Training images before augmentation: 160\n",
      "Validation images: 40\n",
      "Unlabeled images: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training images: 100%|██████████| 160/160 [00:03<00:00, 40.15it/s]\n",
      "Processing validation images: 100%|██████████| 40/40 [00:00<00:00, 69.58it/s]\n",
      "Processing test images: 100%|██████████| 50/50 [00:00<00:00, 75.74it/s]\n",
      "Processing unlabeled images: 100%|██████████| 1000/1000 [00:07<00:00, 126.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final dataset statistics:\n",
      "\n",
      "Image counts:\n",
      "Training images (original): 160\n",
      "Training images (augmented): 480\n",
      "Training images (total): 640\n",
      "Validation images: 40\n",
      "Test images: 50\n",
      "Unlabeled images: 1000\n",
      "\n",
      "Dataset sizes:\n",
      "Training set size: 46.38 MB\n",
      "Validation set size: 2.52 MB\n",
      "Test set size: 3.08 MB\n",
      "Unlabeled set size: 60.48 MB\n",
      "Total dataset size: 112.47 MB\n",
      "\n",
      "Files per directory:\n",
      "train/images: 640 files\n",
      "train/labels: 640 files\n",
      "val/images: 40 files\n",
      "val/labels: 40 files\n",
      "test/images: 50 files\n",
      "test/labels: 50 files\n",
      "unlabeled/images: 1000 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Organizing Datasets\n",
    "def create_directory_structure(base_path):\n",
    "    folders = [\n",
    "        \"train/images\", \"train/labels\",\n",
    "        \"val/images\", \"val/labels\",\n",
    "        \"test/images\", \"test/labels\",\n",
    "        \"unlabeled/images\"\n",
    "    ]\n",
    "    for folder in folders:\n",
    "        os.makedirs(os.path.join(base_path, folder), exist_ok=True)\n",
    "        \n",
    "    return folders # Returns the list of created folder paths\n",
    "\n",
    "# Applying Data augmentation\n",
    "def adjust_annotations(annotations, crop_x, crop_y, crop_w, crop_h, orig_w, orig_h):\n",
    "    \"\"\"Adjust bounding box annotations after cropping.\"\"\"\n",
    "    new_annotations = []\n",
    "    for line in annotations:\n",
    "        class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "        x_abs, y_abs = x_center * orig_w, y_center * orig_h\n",
    "        width, height = width * orig_w, height * orig_h\n",
    "        \n",
    "        x1, y1 = x_abs - width / 2, y_abs - height / 2\n",
    "        x2, y2 = x_abs + width / 2, y_abs + height / 2\n",
    "        \n",
    "        if x1 >= crop_x and y1 >= crop_y and x2 <= crop_x + crop_w and y2 <= crop_y + crop_h:\n",
    "            x1_new, y1_new = x1 - crop_x, y1 - crop_y\n",
    "            x2_new, y2_new = x2 - crop_x, y2 - crop_y\n",
    "            x_center_new = (x1_new + x2_new) / 2 / crop_w\n",
    "            y_center_new = (y1_new + y2_new) / 2 / crop_h\n",
    "            width_new = (x2_new - x1_new) / crop_w\n",
    "            height_new = (y2_new - y1_new) / crop_h\n",
    "            new_annotations.append(f\"{class_id} {x_center_new:.6f} {y_center_new:.6f} {width_new:.6f} {height_new:.6f}\")\n",
    "    return new_annotations\n",
    "\n",
    "def random_crop(image, annotations, crop_size=(224, 224)):\n",
    "    \"\"\"Randomly cropping image and adjusting annotations.\"\"\"\n",
    "    h, w, _ = image.shape\n",
    "    crop_h, crop_w = crop_size\n",
    "    if crop_h > h or crop_w > w:\n",
    "        return image, annotations\n",
    "    x_start = random.randint(0, w - crop_w)\n",
    "    y_start = random.randint(0, h - crop_h)\n",
    "    cropped_image = image[y_start:y_start + crop_h, x_start:x_start + crop_w]\n",
    "    new_annotations = adjust_annotations(annotations, x_start, y_start, crop_w, crop_h, w, h)\n",
    "    return cropped_image, new_annotations\n",
    "\n",
    "def invert_colors(image):\n",
    "    \"\"\"Inverting image colors.\"\"\"\n",
    "    return cv2.bitwise_not(image)\n",
    "\n",
    "def apply_gaussian_blur(image, kernel_size=(5, 5)):\n",
    "    \"\"\"Applying Gaussian blur to image.\"\"\"\n",
    "    return cv2.GaussianBlur(image, kernel_size, 0)\n",
    "    \n",
    "def process_training_set(train_orig, source_paths, dest_paths, augment=True):\n",
    "    \"\"\"Processing training set with optional augmentation.\"\"\"\n",
    "    processed_count = 0\n",
    "    augmented_count = 0\n",
    "    \n",
    "    for img_name in tqdm(train_orig, desc=\"Processing training images\"):\n",
    "        img_path = os.path.join(source_paths['images'], img_name)\n",
    "        ann_path = os.path.join(source_paths['annotations'], img_name.replace(\".jpg\", \".txt\"))\n",
    "        \n",
    "        if not os.path.exists(ann_path):\n",
    "            continue\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        with open(ann_path, \"r\") as f:\n",
    "            annotations = f.readlines()\n",
    "        \n",
    "        # Copy original\n",
    "        shutil.copy(img_path, os.path.join(dest_paths['images'], img_name))\n",
    "        shutil.copy(ann_path, os.path.join(dest_paths['labels'], img_name.replace(\".jpg\", \".txt\")))\n",
    "        processed_count += 1\n",
    "        \n",
    "        if augment:\n",
    "            # Generate augmentations\n",
    "            augmentations = [\n",
    "                random_crop(image, annotations),\n",
    "                (invert_colors(image), annotations),\n",
    "                (apply_gaussian_blur(image), annotations)\n",
    "            ]\n",
    "            \n",
    "            for i, (aug_img, aug_ann) in enumerate(augmentations):\n",
    "                aug_img_name = f\"aug_{i}_{img_name}\"\n",
    "                aug_ann_name = f\"aug_{i}_{img_name.replace('.jpg', '.txt')}\"\n",
    "                cv2.imwrite(os.path.join(dest_paths['images'], aug_img_name), aug_img)\n",
    "                with open(os.path.join(dest_paths['labels'], aug_ann_name), \"w\") as f:\n",
    "                    f.write(\"\\n\".join(aug_ann))\n",
    "                augmented_count += 1\n",
    "                \n",
    "    return processed_count, augmented_count\n",
    "\n",
    "def process_unlabeled_set(unlabeled_images, source_paths, dest_paths):\n",
    "    \"\"\" Process unlabeled images (without annotations). \"\"\"\n",
    "    processed_count = 0\n",
    "    \n",
    "    for img_name in tqdm(unlabeled_images, desc=\"Processing unlabeled images\"):\n",
    "        img_path = os.path.join(source_paths['unlabeled_images'], img_name)  # Corrected this path\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        \n",
    "        # Copy original image to unlabeled folder\n",
    "        shutil.copy(img_path, os.path.join(dest_paths['images'], img_name))\n",
    "        processed_count += 1\n",
    "    \n",
    "    return processed_count\n",
    "\n",
    "# Data Splitting and Statistics \n",
    "def process_dataset_split(image_list, source_paths, dest_paths, split_name=\"\"):\n",
    "    \"\"\"Process dataset split (validation or test).\"\"\"\n",
    "    processed_count = 0\n",
    "    for img_name in tqdm(image_list, desc=f\"Processing {split_name} images\"):\n",
    "        img_path = os.path.join(source_paths['images'], img_name)\n",
    "        ann_path = os.path.join(source_paths['annotations'], img_name.replace(\".jpg\", \".txt\"))\n",
    "        \n",
    "        if not os.path.exists(ann_path):\n",
    "            continue\n",
    "        \n",
    "        shutil.copy(img_path, os.path.join(dest_paths['images'], img_name))\n",
    "        shutil.copy(ann_path, os.path.join(dest_paths['labels'], img_name.replace(\".jpg\", \".txt\")))\n",
    "        processed_count += 1\n",
    "    \n",
    "    return processed_count\n",
    "\n",
    "def get_dir_size(path):\n",
    "    \"\"\"Get directory size in MB.\"\"\"\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size / (1024 * 1024) \n",
    "\n",
    "def main():\n",
    "    # Defining paths for accessing dataset on kaggle\n",
    "    base_path = \"/kaggle/input/weedzip\"\n",
    "    source_paths = {\n",
    "        'labeled_images': os.path.join(base_path, \"labeled/images\"),\n",
    "        'labeled_annotations': os.path.join(base_path, \"labeled/annotations\"),\n",
    "        'test_images': os.path.join(base_path, \"test/images\"),\n",
    "        'test_annotations': os.path.join(base_path, \"test/annotations\"),\n",
    "        'unlabeled_images': os.path.join(base_path, \"unlabeled\")\n",
    "    }\n",
    "    \n",
    "    preprocessed_path = \"/kaggle/working/Preprocessed_img_weed\"\n",
    "    folders = create_directory_structure(preprocessed_path)\n",
    "    \n",
    "    # Split original dataset\n",
    "    all_original_images = os.listdir(source_paths['labeled_images'])\n",
    "    train_orig, val_orig = train_test_split(all_original_images, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Split unlabeled dataset\n",
    "    unlabeled_images = os.listdir(source_paths['unlabeled_images'])\n",
    "    \n",
    "    # Print initial statistics\n",
    "    print(\"\\nOriginal dataset statistics:\")\n",
    "    print(f\"Total original images: {len(all_original_images)}\")\n",
    "    print(f\"Training images before augmentation: {len(train_orig)}\")\n",
    "    print(f\"Validation images: {len(val_orig)}\")\n",
    "    print(f\"Unlabeled images: {len(unlabeled_images)}\")\n",
    "    \n",
    "    # Process training set\n",
    "    train_paths = {\n",
    "        'images': os.path.join(preprocessed_path, \"train/images\"),\n",
    "        'labels': os.path.join(preprocessed_path, \"train/labels\")\n",
    "    }\n",
    "    processed_train, augmented_count = process_training_set(\n",
    "        train_orig,\n",
    "        {'images': source_paths['labeled_images'], 'annotations': source_paths['labeled_annotations']},\n",
    "        train_paths\n",
    "    )\n",
    "    \n",
    "    # Process validation set\n",
    "    val_paths = {\n",
    "        'images': os.path.join(preprocessed_path, \"val/images\"),\n",
    "        'labels': os.path.join(preprocessed_path, \"val/labels\")\n",
    "    }\n",
    "    processed_val = process_dataset_split(\n",
    "        val_orig,\n",
    "        {'images': source_paths['labeled_images'], 'annotations': source_paths['labeled_annotations']},\n",
    "        val_paths,\n",
    "        \"validation\"\n",
    "    )\n",
    "    \n",
    "    # Process test set\n",
    "    test_paths = {\n",
    "        'images': os.path.join(preprocessed_path, \"test/images\"),\n",
    "        'labels': os.path.join(preprocessed_path, \"test/labels\")\n",
    "    }\n",
    "    processed_test = process_dataset_split(\n",
    "        os.listdir(source_paths['test_images']),\n",
    "        {'images': source_paths['test_images'], 'annotations': source_paths['test_annotations']},\n",
    "        test_paths,\n",
    "        \"test\"\n",
    "    )\n",
    "    \n",
    "    # Process unlabeled set\n",
    "    unlabeled_paths = {\n",
    "        'images': os.path.join(preprocessed_path, \"unlabeled/images\")\n",
    "    }\n",
    "    if not os.path.exists(unlabeled_paths['images']):\n",
    "        os.makedirs(unlabeled_paths['images'])\n",
    "    \n",
    "    processed_unlabeled = process_unlabeled_set(\n",
    "        unlabeled_images,\n",
    "        {'unlabeled_images': source_paths['unlabeled_images']},\n",
    "        unlabeled_paths\n",
    "    )\n",
    "    \n",
    "    # Print comprehensive dataset statistics\n",
    "    print(\"\\nFinal dataset statistics:\")\n",
    "    print(\"\\nImage counts:\")\n",
    "    print(f\"Training images (original): {processed_train}\")\n",
    "    print(f\"Training images (augmented): {augmented_count}\")\n",
    "    print(f\"Training images (total): {processed_train + augmented_count}\")\n",
    "    print(f\"Validation images: {processed_val}\")\n",
    "    print(f\"Test images: {processed_test}\")\n",
    "    print(f\"Unlabeled images: {processed_unlabeled}\")\n",
    "    \n",
    "    print(\"\\nDataset sizes:\")\n",
    "    print(f\"Training set size: {get_dir_size(os.path.join(preprocessed_path, 'train')):.2f} MB\")\n",
    "    print(f\"Validation set size: {get_dir_size(os.path.join(preprocessed_path, 'val')):.2f} MB\")\n",
    "    print(f\"Test set size: {get_dir_size(os.path.join(preprocessed_path, 'test')):.2f} MB\")\n",
    "    print(f\"Unlabeled set size: {get_dir_size(os.path.join(preprocessed_path, 'unlabeled')):.2f} MB\")\n",
    "    print(f\"Total dataset size: {get_dir_size(preprocessed_path):.2f} MB\")\n",
    "    \n",
    "    print(\"\\nFiles per directory:\")\n",
    "    for folder in folders:\n",
    "        path = os.path.join(preprocessed_path, folder)\n",
    "        print(f\"{folder}: {len(os.listdir(path))} files\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T07:51:40.922527Z",
     "iopub.status.busy": "2025-02-05T07:51:40.922218Z",
     "iopub.status.idle": "2025-02-05T08:18:10.862773Z",
     "shell.execute_reply": "2025-02-05T08:18:10.861865Z",
     "shell.execute_reply.started": "2025-02-05T07:51:40.922500Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4312, Train Acc: 97.97%, Val Acc: 95.00%\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 20/20 [00:13<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.1179, Train Acc: 98.59%, Val Acc: 95.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.0735, Train Acc: 99.06%, Val Acc: 95.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 20/20 [00:13<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.0310, Train Acc: 99.53%, Val Acc: 95.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 20/20 [00:13<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.0267, Train Acc: 99.53%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.0271, Train Acc: 99.53%, Val Acc: 95.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 20/20 [00:13<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.0246, Train Acc: 99.53%, Val Acc: 95.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.0157, Train Acc: 99.53%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 20/20 [00:14<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.0282, Train Acc: 99.38%, Val Acc: 97.50%\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 20/20 [00:13<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.0167, Train Acc: 99.53%, Val Acc: 95.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 20/20 [00:13<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.0141, Train Acc: 99.53%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.0311, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 20/20 [00:13<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.0168, Train Acc: 99.38%, Val Acc: 95.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 20/20 [00:13<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.0101, Train Acc: 99.53%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.0095, Train Acc: 99.53%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.0089, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 20/20 [00:14<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.0087, Train Acc: 99.53%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 20/20 [00:13<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 0.0081, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 0.0060, Train Acc: 100.00%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 20/20 [00:13<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.0076, Train Acc: 99.84%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 0.0169, Train Acc: 99.84%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 20/20 [00:13<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 0.0129, Train Acc: 99.84%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 20/20 [00:13<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: 0.0120, Train Acc: 99.53%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 20/20 [00:13<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: 0.0170, Train Acc: 99.69%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 20/20 [00:14<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: 0.0694, Train Acc: 99.53%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 20/20 [00:14<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: 0.0614, Train Acc: 99.53%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 20/20 [00:14<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: 0.0404, Train Acc: 99.53%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: 0.0247, Train Acc: 99.53%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 20/20 [00:14<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss: 0.0405, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 20/20 [00:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 0.0296, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Loss: 0.0389, Train Acc: 99.69%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 20/20 [00:15<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Loss: 0.0150, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Loss: 0.0158, Train Acc: 99.38%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 20/20 [00:15<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Loss: 0.0206, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 20/20 [00:15<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Loss: 0.0157, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 20/20 [00:15<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Loss: 0.0313, Train Acc: 99.53%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 20/20 [00:15<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Loss: 0.0208, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 20/20 [00:15<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Loss: 0.0114, Train Acc: 100.00%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 20/20 [00:15<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Loss: 0.0220, Train Acc: 99.53%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss: 0.0351, Train Acc: 99.84%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 20/20 [00:15<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Loss: 0.0343, Train Acc: 99.53%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Loss: 0.0208, Train Acc: 99.53%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 20/20 [00:15<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Loss: 0.0218, Train Acc: 99.69%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Loss: 0.0124, Train Acc: 99.53%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Loss: 0.0108, Train Acc: 99.53%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 20/20 [00:16<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Loss: 0.0381, Train Acc: 99.22%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 20/20 [00:15<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Loss: 0.0424, Train Acc: 99.53%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 20/20 [00:16<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Loss: 0.0278, Train Acc: 100.00%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 20/20 [00:16<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Loss: 0.0149, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.0105, Train Acc: 99.84%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 20/20 [00:16<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, Loss: 0.0084, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, Loss: 0.0129, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 20/20 [00:16<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53, Loss: 0.0557, Train Acc: 99.22%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, Loss: 0.0281, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 20/20 [00:16<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55, Loss: 0.0227, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, Loss: 0.0198, Train Acc: 100.00%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, Loss: 0.0129, Train Acc: 99.53%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, Loss: 0.0108, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 20/20 [00:16<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59, Loss: 0.0124, Train Acc: 99.38%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Loss: 0.0084, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 20/20 [00:16<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, Loss: 0.0082, Train Acc: 99.22%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 20/20 [00:16<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, Loss: 0.0072, Train Acc: 99.84%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, Loss: 0.0117, Train Acc: 99.69%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 20/20 [00:16<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64, Loss: 0.0114, Train Acc: 99.38%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, Loss: 0.0100, Train Acc: 99.53%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 20/20 [00:16<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, Loss: 0.0081, Train Acc: 99.69%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, Loss: 0.0114, Train Acc: 99.38%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 20/20 [00:16<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, Loss: 0.0094, Train Acc: 99.69%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, Loss: 0.0204, Train Acc: 99.53%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, Loss: 0.0128, Train Acc: 99.69%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, Loss: 0.0104, Train Acc: 99.84%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72, Loss: 0.0084, Train Acc: 99.84%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73, Loss: 0.0068, Train Acc: 99.69%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, Loss: 0.0149, Train Acc: 99.53%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Loss: 0.0275, Train Acc: 99.84%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, Loss: 0.0250, Train Acc: 99.69%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, Loss: 0.0563, Train Acc: 99.22%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, Loss: 0.0373, Train Acc: 99.53%, Val Acc: 92.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, Loss: 0.0157, Train Acc: 99.84%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, Loss: 0.0247, Train Acc: 99.53%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, Loss: 0.0287, Train Acc: 99.84%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, Loss: 0.0221, Train Acc: 99.69%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83, Loss: 0.0180, Train Acc: 99.53%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, Loss: 0.0138, Train Acc: 99.53%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, Loss: 0.0172, Train Acc: 99.69%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, Loss: 0.0252, Train Acc: 99.53%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, Loss: 0.0101, Train Acc: 99.53%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88, Loss: 0.0080, Train Acc: 99.69%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, Loss: 0.0086, Train Acc: 99.38%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Loss: 0.0185, Train Acc: 99.38%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91, Loss: 0.0212, Train Acc: 99.84%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, Loss: 0.0210, Train Acc: 99.84%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, Loss: 0.0263, Train Acc: 99.84%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, Loss: 0.0346, Train Acc: 99.69%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, Loss: 0.0403, Train Acc: 99.84%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, Loss: 0.0225, Train Acc: 99.69%, Val Acc: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, Loss: 0.0366, Train Acc: 99.69%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 20/20 [00:16<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, Loss: 0.0134, Train Acc: 99.53%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, Loss: 0.0164, Train Acc: 99.84%, Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 20/20 [00:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.0135, Train Acc: 99.69%, Val Acc: 87.50%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Accessing the preproceesd dataset for training\n",
    "PREPROCESSED_PATH = r\"/kaggle/working/Preprocessed_img_weed\"\n",
    "\n",
    "# Class for loading weed classification images and labels.\n",
    "class WeedDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir=None, transform=None, is_unlabeled=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.is_unlabeled = is_unlabeled\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def read_label_file(self, label_path):\n",
    "        try:\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                # Filter out empty lines and whitespace\n",
    "                lines = [line.strip() for line in lines if line.strip()]\n",
    "                \n",
    "                if not lines:\n",
    "                    return 1 \n",
    "                \n",
    "                for line in lines:\n",
    "                    parts = line.split()\n",
    "                    if parts and parts[0] == '0': \n",
    "                        return 0  # Weed\n",
    "                return 1  # Crop\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading label file {label_path}: {str(e)}\")\n",
    "            return 1  # Default to crop in case of errors\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            # Create a blank image in case of error\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.is_unlabeled:\n",
    "            return image.to(device)\n",
    "            \n",
    "        label_path = os.path.join(self.label_dir, img_name.replace('.jpg', '.txt'))\n",
    "        label = torch.tensor(self.read_label_file(label_path))\n",
    "            \n",
    "        return image.to(device), label.to(device)\n",
    "    \n",
    "#                          * MEAN TEACHER METHOD (MTM) OVERVIEW *\n",
    "\n",
    "# Implementing a semi-supervised learning approach for weed detection using a Student-teacher framework with ResNet-50.\n",
    "#\n",
    "#  - The **student model** learns from both labeled and pseudo-labeled data.\n",
    "#  - The **teacher model** provides pseudo-labels for unlabeled data.\n",
    "#  - The teacher model is updated using **Exponential Moving Average (EMA)\n",
    "\n",
    "class SemiSupervisedWeedDetector:\n",
    "    def __init__(self, confidence_threshold=0.85):\n",
    "        self.student_model = self._create_model().to(device)\n",
    "        self.teacher_model = self._create_model().to(device)\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.ema_decay = 0.999\n",
    "        \n",
    "    def _create_model(self):\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "        return model\n",
    "        \n",
    "    def update_teacher(self):\n",
    "        for teacher_param, student_param in zip(self.teacher_model.parameters(), \n",
    "                                                self.student_model.parameters()):\n",
    "            teacher_param.data = self.ema_decay * teacher_param.data + (1 - self.ema_decay) * student_param.data\n",
    "    \n",
    "    def train_step(self, labeled_batch, unlabeled_batch):\n",
    "        images, labels = labeled_batch\n",
    "        unlabeled_images = unlabeled_batch\n",
    "        \n",
    "        outputs = self.student_model(images)\n",
    "        supervised_loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pseudo_logits = self.teacher_model(unlabeled_images)\n",
    "            pseudo_probs = torch.softmax(pseudo_logits, dim=1)\n",
    "            confidence, pseudo_labels = torch.max(pseudo_probs, dim=1)\n",
    "        \n",
    "        mask = confidence > self.confidence_threshold\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            unsupervised_loss = nn.CrossEntropyLoss()(self.student_model(unlabeled_images[mask]), pseudo_labels[mask])\n",
    "        else:\n",
    "            unsupervised_loss = torch.tensor(0.0).to(device)\n",
    "        \n",
    "        total_loss = supervised_loss + unsupervised_loss\n",
    "        return total_loss\n",
    "    \n",
    "    def train(self, labeled_loader, unlabeled_loader, val_loader, num_epochs=100):\n",
    "        optimizer = torch.optim.AdamW(self.student_model.parameters(), lr=1e-4)\n",
    "        best_val_acc = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.student_model.train()\n",
    "            epoch_loss = 0\n",
    "            correct, total = 0, 0\n",
    "            \n",
    "            pbar = tqdm(zip(labeled_loader, unlabeled_loader), total=min(len(labeled_loader), len(unlabeled_loader)))\n",
    "            for labeled_batch, unlabeled_batch in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                loss = self.train_step(labeled_batch, unlabeled_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                self.update_teacher()\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                images, labels = labeled_batch\n",
    "                outputs = self.student_model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                pbar.set_description(f\"Epoch {epoch+1}\")\n",
    "            \n",
    "            train_acc = 100 * correct / total\n",
    "            val_acc = self.evaluate(val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(labeled_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(self.student_model.state_dict(), 'best_model.pth')\n",
    "                print(\"Best model saved!\")\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        self.student_model.eval()\n",
    "        correct, total = 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                outputs = self.student_model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        return 100 * correct / total\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "# Define transformations for training data\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.RandomRotation(30),\n",
    "    T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = WeedDataset(os.path.join(PREPROCESSED_PATH, \"train/images\"), os.path.join(PREPROCESSED_PATH, \"train/labels\"), train_transform)\n",
    "val_dataset = WeedDataset(os.path.join(PREPROCESSED_PATH, \"val/images\"), os.path.join(PREPROCESSED_PATH, \"val/labels\"), val_transform)\n",
    "unlabeled_dataset = WeedDataset(os.path.join(PREPROCESSED_PATH, \"unlabeled/images\"), transform=train_transform, is_unlabeled=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = SemiSupervisedWeedDetector()\n",
    "model.train(train_loader, unlabeled_loader, val_loader, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T08:18:17.157046Z",
     "iopub.status.busy": "2025-02-05T08:18:17.156740Z",
     "iopub.status.idle": "2025-02-05T08:18:17.390854Z",
     "shell.execute_reply": "2025-02-05T08:18:17.389814Z",
     "shell.execute_reply.started": "2025-02-05T08:18:17.157023Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at semi_supervised_weed_model.pth\n"
     ]
    }
   ],
   "source": [
    "MODEL_SAVE_PATH = \"semi_supervised_weed_model.pth\"\n",
    "torch.save(model.student_model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Model saved at {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T08:20:24.516968Z",
     "iopub.status.busy": "2025-02-05T08:20:24.516589Z",
     "iopub.status.idle": "2025-02-05T08:20:25.110485Z",
     "shell.execute_reply": "2025-02-05T08:20:25.109788Z",
     "shell.execute_reply.started": "2025-02-05T08:20:24.516939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-ea01e88b1ed1>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.student_model.load_state_dict(torch.load(r'/kaggle/working/best_model.pth'))\n",
      "Evaluating Test Accuracy: 100%|██████████| 2/2 [00:00<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test Accuracy: 100.00%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[24  0]\n",
      " [ 0 26]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Weed       1.00      1.00      1.00        24\n",
      "        Weed       1.00      1.00      1.00        26\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path for test dataset\n",
    "TEST_IMAGE_PATH = r\"/kaggle/working/Preprocessed_img_weed/test/images\"\n",
    "TEST_LABEL_PATH = r\"/kaggle/working/Preprocessed_img_weed/test/labels\"\n",
    "\n",
    "# Loading the best trained model\n",
    "model.student_model.load_state_dict(torch.load(r'/kaggle/working/best_model.pth'))\n",
    "model.student_model = model.student_model.to(device)  \n",
    "model.student_model.eval()\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "# Defining the test dataset with labels using the new read_label_file method\n",
    "class TestDatasetWithLabels(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def read_label_file(self, label_path):\n",
    "        try:\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                lines = [line.strip() for line in lines if line.strip()]\n",
    "                \n",
    "                if not lines:\n",
    "                    return 1 \n",
    "                \n",
    "                for line in lines:\n",
    "                    parts = line.split()\n",
    "                    if parts and parts[0] == '0':  \n",
    "                        return 0  \n",
    "                return 1  # Crop\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading label file {label_path}: {str(e)}\")\n",
    "            return 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Fetch an image and its label.\"\"\"\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label_path = os.path.join(self.label_dir, img_name.replace('.jpg', '.txt'))\n",
    "        label = torch.tensor(self.read_label_file(label_path))  \n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Defining test transform (same as validation)\n",
    "test_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_dataset = TestDatasetWithLabels(TEST_IMAGE_PATH, TEST_LABEL_PATH, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Evaluating Test Accuracy\"):\n",
    "        images, labels = images.to(device), labels.to(device) \n",
    "        outputs = model.student_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Computing accuracy\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"✅ Test Accuracy: {test_accuracy:.2f}%\")\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Weed', 'Weed'], yticklabels=['Non-Weed', 'Weed'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Non-Weed', 'Weed'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Observations**\n",
    "*By simply fine-tuning a ResNet-50 model for classification task without using MTM method we achieved an accuracy of 74% on test dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Without data augmentation on training set we achieved F1-Score of 0.98.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For 50 epochs we obtained accuracy of 98%.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*With 100 epochs and appropriate data augmentation we achieved F1-Score of 1.*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6569314,
     "sourceId": 10611371,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
