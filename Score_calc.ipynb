{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10611371,"sourceType":"datasetVersion","datasetId":6569314},{"sourceId":251010,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":213487,"modelId":235133}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport shutil\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as T\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:27:12.829586Z","iopub.execute_input":"2025-02-05T18:27:12.829937Z","iopub.status.idle":"2025-02-05T18:27:19.896081Z","shell.execute_reply.started":"2025-02-05T18:27:12.829914Z","shell.execute_reply":"2025-02-05T18:27:19.895334Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## APPLYING SAME AUGMENTATION ON THE DATA AS WAS USED TO TRAIN THE MODEL ##","metadata":{}},{"cell_type":"code","source":"#-----------------------------------------------------------------------------------\n#  Data Organistation\ndef create_directory_structure(base_path):\n    \"\"\"Create the necessary directory structure for the dataset.\"\"\"\n\n    folders = [\n        \"train/images\", \"train/labels\",\n        \"val/images\", \"val/labels\",\n        \"test/images\", \"test/labels\",\n        \"unlabeled/images\"\n    ]\n\n    for folder in folders:\n        os.makedirs(os.path.join(base_path, folder), exist_ok=True)\n        \n    return folders\n#------------------------------------------------------------------------------------\n\n# Data augmentation\ndef adjust_annotations(annotations, crop_x, crop_y, crop_w, crop_h, orig_w, orig_h):\n    \"\"\"Adjust bounding box annotations after cropping.\"\"\"\n    new_annotations = []\n    for line in annotations:\n        class_id, x_center, y_center, width, height = map(float, line.strip().split())\n        x_abs, y_abs = x_center * orig_w, y_center * orig_h\n        width, height = width * orig_w, height * orig_h\n        \n        x1, y1 = x_abs - width / 2, y_abs - height / 2\n        x2, y2 = x_abs + width / 2, y_abs + height / 2\n        \n        if x1 >= crop_x and y1 >= crop_y and x2 <= crop_x + crop_w and y2 <= crop_y + crop_h:\n            x1_new, y1_new = x1 - crop_x, y1 - crop_y\n            x2_new, y2_new = x2 - crop_x, y2 - crop_y\n            x_center_new = (x1_new + x2_new) / 2 / crop_w\n            y_center_new = (y1_new + y2_new) / 2 / crop_h\n            width_new = (x2_new - x1_new) / crop_w\n            height_new = (y2_new - y1_new) / crop_h\n            new_annotations.append(f\"{class_id} {x_center_new:.6f} {y_center_new:.6f} {width_new:.6f} {height_new:.6f}\")\n    return new_annotations\n\ndef random_crop(image, annotations, crop_size=(224, 224)):\n    \"\"\"Randomly crop image and adjust annotations.\"\"\"\n    h, w, _ = image.shape\n    crop_h, crop_w = crop_size\n    if crop_h > h or crop_w > w:\n        return image, annotations\n    x_start = random.randint(0, w - crop_w)\n    y_start = random.randint(0, h - crop_h)\n    cropped_image = image[y_start:y_start + crop_h, x_start:x_start + crop_w]\n    new_annotations = adjust_annotations(annotations, x_start, y_start, crop_w, crop_h, w, h)\n    return cropped_image, new_annotations\n\ndef invert_colors(image):\n    \"\"\"Invert image colors.\"\"\"\n    return cv2.bitwise_not(image)\n\ndef apply_gaussian_blur(image, kernel_size=(5, 5)):\n    \"\"\"Apply Gaussian blur to image.\"\"\"\n    return cv2.GaussianBlur(image, kernel_size, 0)\n\n    \ndef process_training_set(train_orig, source_paths, dest_paths, augment=True):\n    \"\"\"Process training set with optional augmentation.\"\"\"\n    processed_count = 0\n    augmented_count = 0\n    \n    for img_name in tqdm(train_orig, desc=\"Processing training images\"):\n        img_path = os.path.join(source_paths['images'], img_name)\n        ann_path = os.path.join(source_paths['annotations'], img_name.replace(\".jpg\", \".txt\"))\n        \n        if not os.path.exists(ann_path):\n            continue\n        \n        image = cv2.imread(img_path)\n        with open(ann_path, \"r\") as f:\n            annotations = f.readlines()\n        \n        \n        shutil.copy(img_path, os.path.join(dest_paths['images'], img_name))\n        shutil.copy(ann_path, os.path.join(dest_paths['labels'], img_name.replace(\".jpg\", \".txt\")))\n        processed_count += 1\n        \n        if augment:\n            augmentations = [\n                random_crop(image, annotations),\n                (invert_colors(image), annotations),\n                (apply_gaussian_blur(image), annotations)\n            ]\n            \n            for i, (aug_img, aug_ann) in enumerate(augmentations):\n                aug_img_name = f\"aug_{i}_{img_name}\"\n                aug_ann_name = f\"aug_{i}_{img_name.replace('.jpg', '.txt')}\"\n                cv2.imwrite(os.path.join(dest_paths['images'], aug_img_name), aug_img)\n                with open(os.path.join(dest_paths['labels'], aug_ann_name), \"w\") as f:\n                    f.write(\"\\n\".join(aug_ann))\n                augmented_count += 1\n                \n    return processed_count, augmented_count\n\ndef process_unlabeled_set(unlabeled_images, source_paths, dest_paths):\n    \"\"\"Process unlabeled images (without annotations).\"\"\"\n    processed_count = 0\n    \n    for img_name in tqdm(unlabeled_images, desc=\"Processing unlabeled images\"):\n        img_path = os.path.join(source_paths['unlabeled_images'], img_name) \n        if not os.path.exists(img_path):\n            continue\n        \n        \n        shutil.copy(img_path, os.path.join(dest_paths['images'], img_name))\n        processed_count += 1\n    \n    return processed_count\n#-----------------------------------------------------------------------------------\n\n# Data Splitting and Statistics \ndef process_dataset_split(image_list, source_paths, dest_paths, split_name=\"\"):\n    \"\"\"Process dataset split (validation or test).\"\"\"\n    processed_count = 0\n    for img_name in tqdm(image_list, desc=f\"Processing {split_name} images\"):\n        img_path = os.path.join(source_paths['images'], img_name)\n        ann_path = os.path.join(source_paths['annotations'], img_name.replace(\".jpg\", \".txt\"))\n        \n        if not os.path.exists(ann_path):\n            continue\n        \n        shutil.copy(img_path, os.path.join(dest_paths['images'], img_name))\n        shutil.copy(ann_path, os.path.join(dest_paths['labels'], img_name.replace(\".jpg\", \".txt\")))\n        processed_count += 1\n    \n    return processed_count\n\ndef get_dir_size(path):\n    \"\"\"Get directory size in MB.\"\"\"\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            total_size += os.path.getsize(fp)\n    return total_size / (1024 * 1024)\n#------------------------------------------------------------------------------------    \n\ndef main():\n    # Define paths\n    base_path = \"/kaggle/input/weedzip\"\n    source_paths = {\n        'labeled_images': os.path.join(base_path, \"labeled/images\"),\n        'labeled_annotations': os.path.join(base_path, \"labeled/annotations\"),\n        'test_images': os.path.join(base_path, \"test/images\"),\n        'test_annotations': os.path.join(base_path, \"test/annotations\"),\n        'unlabeled_images': os.path.join(base_path, \"unlabeled\")\n    }\n    \n    preprocessed_path = \"/kaggle/working/Preprocessed_img_weed\"\n    folders = create_directory_structure(preprocessed_path)\n    \n    # Split original dataset\n    all_original_images = os.listdir(source_paths['labeled_images'])\n    train_orig, val_orig = train_test_split(all_original_images, test_size=0.2, random_state=42)\n    \n    # Split unlabeled dataset\n    unlabeled_images = os.listdir(source_paths['unlabeled_images'])\n    \n    # Print initial statistics\n    print(\"\\nOriginal dataset statistics:\")\n    print(f\"Total original images: {len(all_original_images)}\")\n    print(f\"Training images before augmentation: {len(train_orig)}\")\n    print(f\"Validation images: {len(val_orig)}\")\n    print(f\"Unlabeled images: {len(unlabeled_images)}\")\n    \n    # Process training set\n    train_paths = {\n        'images': os.path.join(preprocessed_path, \"train/images\"),\n        'labels': os.path.join(preprocessed_path, \"train/labels\")\n    }\n    processed_train, augmented_count = process_training_set(\n        train_orig,\n        {'images': source_paths['labeled_images'], 'annotations': source_paths['labeled_annotations']},\n        train_paths\n    )\n    \n    # Process validation set\n    val_paths = {\n        'images': os.path.join(preprocessed_path, \"val/images\"),\n        'labels': os.path.join(preprocessed_path, \"val/labels\")\n    }\n    processed_val = process_dataset_split(\n        val_orig,\n        {'images': source_paths['labeled_images'], 'annotations': source_paths['labeled_annotations']},\n        val_paths,\n        \"validation\"\n    )\n    \n    # Process test set\n    test_paths = {\n        'images': os.path.join(preprocessed_path, \"test/images\"),\n        'labels': os.path.join(preprocessed_path, \"test/labels\")\n    }\n    processed_test = process_dataset_split(\n        os.listdir(source_paths['test_images']),\n        {'images': source_paths['test_images'], 'annotations': source_paths['test_annotations']},\n        test_paths,\n        \"test\"\n    )\n    \n    # Process unlabeled set\n    unlabeled_paths = {\n        'images': os.path.join(preprocessed_path, \"unlabeled/images\")\n    }\n    if not os.path.exists(unlabeled_paths['images']):\n        os.makedirs(unlabeled_paths['images'])\n    \n    processed_unlabeled = process_unlabeled_set(\n        unlabeled_images,\n        {'unlabeled_images': source_paths['unlabeled_images']},\n        unlabeled_paths\n    )\n    \n    # Print comprehensive dataset statistics\n    print(\"\\nFinal dataset statistics:\")\n    print(\"\\nImage counts:\")\n    print(f\"Training images (original): {processed_train}\")\n    print(f\"Training images (augmented): {augmented_count}\")\n    print(f\"Training images (total): {processed_train + augmented_count}\")\n    print(f\"Validation images: {processed_val}\")\n    print(f\"Test images: {processed_test}\")\n    print(f\"Unlabeled images: {processed_unlabeled}\")\n    \n    print(\"\\nDataset sizes:\")\n    print(f\"Training set size: {get_dir_size(os.path.join(preprocessed_path, 'train')):.2f} MB\")\n    print(f\"Validation set size: {get_dir_size(os.path.join(preprocessed_path, 'val')):.2f} MB\")\n    print(f\"Test set size: {get_dir_size(os.path.join(preprocessed_path, 'test')):.2f} MB\")\n    print(f\"Unlabeled set size: {get_dir_size(os.path.join(preprocessed_path, 'unlabeled')):.2f} MB\")\n    print(f\"Total dataset size: {get_dir_size(preprocessed_path):.2f} MB\")\n    \n    print(\"\\nFiles per directory:\")\n    for folder in folders:\n        path = os.path.join(preprocessed_path, folder)\n        print(f\"{folder}: {len(os.listdir(path))} files\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:27:19.897210Z","iopub.execute_input":"2025-02-05T18:27:19.897628Z","iopub.status.idle":"2025-02-05T18:27:33.987031Z","shell.execute_reply.started":"2025-02-05T18:27:19.897587Z","shell.execute_reply":"2025-02-05T18:27:33.986163Z"}},"outputs":[{"name":"stdout","text":"\nOriginal dataset statistics:\nTotal original images: 200\nTraining images before augmentation: 160\nValidation images: 40\nUnlabeled images: 1000\n","output_type":"stream"},{"name":"stderr","text":"Processing training images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:03<00:00, 41.86it/s]\nProcessing validation images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 69.43it/s]\nProcessing test images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 73.51it/s]\nProcessing unlabeled images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:08<00:00, 115.97it/s]","output_type":"stream"},{"name":"stdout","text":"\nFinal dataset statistics:\n\nImage counts:\nTraining images (original): 160\nTraining images (augmented): 480\nTraining images (total): 640\nValidation images: 40\nTest images: 50\nUnlabeled images: 1000\n\nDataset sizes:\nTraining set size: 46.42 MB\nValidation set size: 2.52 MB\nTest set size: 3.08 MB\nUnlabeled set size: 60.48 MB\nTotal dataset size: 112.51 MB\n\nFiles per directory:\ntrain/images: 640 files\ntrain/labels: 640 files\nval/images: 40 files\nval/labels: 40 files\ntest/images: 50 files\ntest/labels: 50 files\nunlabeled/images: 1000 files\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### The data is saved in the directory named Preprocessed_img_weed ###","metadata":{}},{"cell_type":"markdown","source":"## Calculating F1-Score and mAP[50:95] ##","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\n\n# ----------------------------\n# 1. Create data.yaml File\n# ----------------------------\nyaml_content = \"\"\"\npath: /kaggle/working/Preprocessed_img_weed\ntrain: train\nval: val\ntest: test\n\nnc: 2\nnames: ['weed', 'crop']\n\"\"\"\n\ndata_yaml_path = r'/kaggle/working/Preprocessed_img_weed/data.yaml'\nif not os.path.exists(data_yaml_path):\n    with open(data_yaml_path, \"w\") as f:\n        f.write(yaml_content)\n    print(\"data.yaml file created successfully!\")\n# ----------------------------\n# 2. Load Models and Define Transform\n# ----------------------------\nYOLO_MODEL_PATH = r'/kaggle/input/yolo/pytorch/default/3/best (3).pt'   #give your YOLO model path\nCLF_MODEL_PATH  = r'/kaggle/input/yolo/pytorch/default/3/best_model (4).pth'  #give your resnet model\nTEST_IMAGE_DIR  = r'/kaggle/input/weedzip/test/images'\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndet_model = YOLO(YOLO_MODEL_PATH)\n\ndef load_resnet50():\n    model = models.resnet50(weights=None)\n    model.fc = nn.Linear(model.fc.in_features, 2)\n    return model.to(device)\n\nclf_model = load_resnet50()\nclf_model.load_state_dict(torch.load(CLF_MODEL_PATH, map_location=device))\nclf_model.eval()\n\nclf_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n# ------------------------------------------|\n# 3. Process All Images and Collect Metrics |\n# ----------------------------------------- |\nall_true_labels = [] \nall_pred_labels = []  \nall_detections = []   \nall_ground_truths = []  \n\nMIN_AREA_RATIO = 0.05\n\nfor image_filename in os.listdir(TEST_IMAGE_DIR):\n    image_path = os.path.join(TEST_IMAGE_DIR, image_filename)\n    image = cv2.imread(image_path)\n    if image is None:\n        continue\n    orig_image = image.copy()\n    img_h, img_w = image.shape[:2]\n\n    results = det_model(image)\n    for result in results:\n        boxes = result.boxes.xyxy.cpu().numpy()\n        scores = result.boxes.conf.cpu().numpy()\n        \n        for box, score in zip(boxes, scores):\n            x1, y1, x2, y2 = box.astype(int)\n            box_area = (x2 - x1) * (y2 - y1)\n            if box_area < MIN_AREA_RATIO * (img_w * img_h):\n                continue\n            if (x2-x1 < 100 or y2-y1 < 100) and ((x2-x1)/(y2-y1) < 0.5 or (x2-x1)/(y2-y1) > 2):\n                continue\n            \n            crop = orig_image[y1:y2, x1:x2]\n            if crop.size == 0:\n                continue\n            \n            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n            crop_pil = Image.fromarray(crop_rgb)\n            input_tensor = clf_transform(crop_pil).unsqueeze(0).to(device)\n            \n            with torch.no_grad():\n                outputs = clf_model(input_tensor)\n                pred_class = int(torch.argmax(outputs, dim=1).item())\n            \n            all_pred_labels.append(pred_class)\n            true_label = 0 if \"weed\" in image_filename else 1\n            all_true_labels.append(true_label)\n            \n            all_detections.append([x1, y1, x2, y2, score])\n            all_ground_truths.append(true_label)\n# ----------------------------\n# 4. Calculate Metrics\n# ----------------------------\n# F1-score for classification\nf1 = f1_score(all_true_labels, all_pred_labels)\nprint(f\"F1 Score (ResNet50): {f1:.4f}\")\n# mAP calculation for YOLO\nmetrics = det_model.val(data=data_yaml_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:38:31.805388Z","iopub.execute_input":"2025-02-05T18:38:31.805771Z","iopub.status.idle":"2025-02-05T18:38:38.115221Z","shell.execute_reply.started":"2025-02-05T18:38:31.805742Z","shell.execute_reply":"2025-02-05T18:38:38.114304Z"}},"outputs":[{"name":"stdout","text":"\n0: 640x640 1 weed, 16.2ms\nSpeed: 2.8ms preprocess, 16.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 16.2ms\nSpeed: 2.6ms preprocess, 16.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 16.2ms\nSpeed: 2.6ms preprocess, 16.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 16.2ms\nSpeed: 2.4ms preprocess, 16.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 13.6ms\nSpeed: 2.4ms preprocess, 13.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 11.7ms\nSpeed: 2.4ms preprocess, 11.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 11.7ms\nSpeed: 2.6ms preprocess, 11.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 weeds, 11.7ms\nSpeed: 2.4ms preprocess, 11.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 weeds, 11.7ms\nSpeed: 2.3ms preprocess, 11.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 11.7ms\nSpeed: 2.4ms preprocess, 11.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 10.9ms\nSpeed: 2.4ms preprocess, 10.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 8.7ms\nSpeed: 2.4ms preprocess, 8.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 8.7ms\nSpeed: 2.4ms preprocess, 8.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 3 backgrounds, 8.7ms\nSpeed: 2.4ms preprocess, 8.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 8.6ms\nSpeed: 2.4ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 8.6ms\nSpeed: 2.5ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 8.6ms\nSpeed: 2.5ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 backgrounds, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 7.7ms\nSpeed: 2.5ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 weeds, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 weeds, 3 backgrounds, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 weeds, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 14 weeds, 7.5ms\nSpeed: 2.2ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 5 weeds, 7.5ms\nSpeed: 2.6ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 7.5ms\nSpeed: 3.0ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 backgrounds, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 weeds, 7.6ms\nSpeed: 2.5ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 7.6ms\nSpeed: 2.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 5 backgrounds, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 backgrounds, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 7.4ms\nSpeed: 2.4ms preprocess, 7.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 7.5ms\nSpeed: 2.3ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 7.5ms\nSpeed: 2.5ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 3 weeds, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 background, 7.5ms\nSpeed: 2.4ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 weed, 7.4ms\nSpeed: 2.3ms preprocess, 7.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\nF1 Score (ResNet50): 0.7600\nUltralytics 8.3.71 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15095MiB)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/Preprocessed_img_weed/val/labels.cache... 40 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<?, ?it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         40         82      0.915      0.951      0.967      0.881\n                  weed         14         40      0.946          1       0.98      0.924\n            background         26         42      0.883      0.901      0.953      0.838\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n","output_type":"stream"},{"name":"stdout","text":"Speed: 5.2ms preprocess, 9.9ms inference, 0.0ms loss, 1.7ms postprocess per image\nResults saved to \u001b[1mruns/detect/val3\u001b[0m\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## As seen F1-Score = 0.76 and mAP[50:95] = 0.881 ##\n# EVALUTION_SCORE = 0.82 #","metadata":{}}]}